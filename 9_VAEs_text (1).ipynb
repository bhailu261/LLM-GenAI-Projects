{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FicOT9D8XlH",
        "outputId": "ad2ad934-8756-4a0b-889a-a80f6f3cdb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/611.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HBZa5HtkKUe",
        "outputId": "4848d5fa-62f9-4198-e034-9d359c1aa14e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import ELU, Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from scipy import spatial\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "import csv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rpWno_9p-25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8BWTjKtkKUf"
      },
      "source": [
        "### Directories and text loading\n",
        "Initially we will set the main directories and some variables regarding the characteristics of our texts.\n",
        "We set the maximum sequence length to 15, the maximun number of words in our vocabulary to 12000 and we will use 50-dimensional embeddings. Finally we load our texts from a csv. The text file is the train file of the Quora Kaggle challenge containing around 808000 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bovnwb2fnFKN",
        "outputId": "e009ad42-9626-4fa6-fd29-90603652a6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing get_data.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f quora.csv ]; then\n",
        "  wget -O quora.csv https://www.dropbox.com/scl/fi/wxvgvw6y48whtuvcx1quq/questions.csv?rlkey=03yokqc36sht66me4jgzmbu12&dl=0\n",
        "fi\n",
        "\n",
        "if [ ! -f glove.6B.100d.txt ]; then\n",
        "  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hflxDcrnUrX",
        "outputId": "cdd31033-52ca-4e00-a818-745f6502fdab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-29 06:22:05--  https://www.dropbox.com/scl/fi/wxvgvw6y48whtuvcx1quq/questions.csv?rlkey=03yokqc36sht66me4jgzmbu12\n",
            "--2024-02-29 06:22:05--  https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... 162.125.80.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "connected.\n",
            "HTTP request sent, awaiting response... HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/dl1vswq2sz5f1ws/glove.6B.100d.txt [following]\n",
            "--2024-02-29 06:22:06--  https://www.dropbox.com/s/raw/dl1vswq2sz5f1ws/glove.6B.100d.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca572de1e44b7e7b293dd142323.dl.dropboxusercontent.com/cd/0/inline/COKLRIjz_WWdaBFB_Atu_c_fi63xB7_fOd1qPK7Fi3i1HgXK6FXmJ4pjjBUdS1rTydD7cG3B7OG2aJETvXrxP4oYkDnlFI0dsBQdnmZB3_88MXWXBcVJEL3oo4gKtwrylWQAXcHO32U15MF35HFFm6y2/file# [following]\n",
            "--2024-02-29 06:22:06--  https://uca572de1e44b7e7b293dd142323.dl.dropboxusercontent.com/cd/0/inline/COKLRIjz_WWdaBFB_Atu_c_fi63xB7_fOd1qPK7Fi3i1HgXK6FXmJ4pjjBUdS1rTydD7cG3B7OG2aJETvXrxP4oYkDnlFI0dsBQdnmZB3_88MXWXBcVJEL3oo4gKtwrylWQAXcHO32U15MF35HFFm6y2/file\n",
            "Resolving uca572de1e44b7e7b293dd142323.dl.dropboxusercontent.com (uca572de1e44b7e7b293dd142323.dl.dropboxusercontent.com)... 302 Found\n",
            "Location: https://ucc2ab80316758465eba8c3e0706.dl.dropboxusercontent.com/cd/0/inline/COL8mnffJDI0ffIyzqqwVPyv8oKVgIan-iCRBuFkB7kR2oUCt-AYDoAQ6ipFhxuwSX5gLYQBoXEkXczF3B_Z9oxMYH9FuYbX5cbzmKVksQrC9cTyF0aEYCjAUWX5C2nJb8VCWRddyk-yqoxwdanTES7F/file# [following]\n",
            "--2024-02-29 06:22:06--  https://ucc2ab80316758465eba8c3e0706.dl.dropboxusercontent.com/cd/0/inline/COL8mnffJDI0ffIyzqqwVPyv8oKVgIan-iCRBuFkB7kR2oUCt-AYDoAQ6ipFhxuwSX5gLYQBoXEkXczF3B_Z9oxMYH9FuYbX5cbzmKVksQrC9cTyF0aEYCjAUWX5C2nJb8VCWRddyk-yqoxwdanTES7F/file\n",
            "Resolving ucc2ab80316758465eba8c3e0706.dl.dropboxusercontent.com (ucc2ab80316758465eba8c3e0706.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uca572de1e44b7e7b293dd142323.dl.dropboxusercontent.com (uca572de1e44b7e7b293dd142323.dl.dropboxusercontent.com)|162.125.80.15|:443... 162.125.64.15, 2620:100:6035:15::a27d:550f\n",
            "Connecting to ucc2ab80316758465eba8c3e0706.dl.dropboxusercontent.com (ucc2ab80316758465eba8c3e0706.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\n",
            "HTTP request sent, awaiting response... connected.\n",
            "200 OK\n",
            "Length: 347117594 (331M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.txt’\n",
            "\n",
            "glove.6B.100d.txt     6%[>                   ]  20.01M  20.1MB/s               200 OK\n",
            "Length: 60747409 (58M) [text/plain]\n",
            "Saving to: ‘quora.csv’\n",
            "\n",
            "quora.csv           100%[===================>]  57.93M  10.8MB/s    in 6.8s    \n",
            "\n",
            "2024-02-29 06:22:15 (8.53 MB/s) - ‘quora.csv’ saved [60747409/60747409]\n",
            "\n",
            "glove.6B.100d.txt   100%[===================>] 331.04M  28.6MB/s    in 12s     \n",
            "\n",
            "2024-02-29 06:22:19 (27.8 MB/s) - ‘glove.6B.100d.txt’ saved [347117594/347117594]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMP_8GBwkKUh",
        "outputId": "139e7f5d-a33d-413e-a040-a8ac08ec665a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 808702 texts in train.csv\n"
          ]
        }
      ],
      "source": [
        "TRAIN_DATA_FILE = './quora.csv'\n",
        "GLOVE_EMBEDDING = './glove.6B.100d.txt'\n",
        "VALIDATION_SPLIT = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 15\n",
        "MAX_NB_WORDS = 12000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "texts = []\n",
        "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        texts.append(values[3])\n",
        "        texts.append(values[4])\n",
        "print('Found %s texts in train.csv' % len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pwaMsM6ungYu",
        "outputId": "f87fef4c-fbb8-486e-f104-820f0bc4d0e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-gKWU_noY2"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I2cAMkSnp_K",
        "outputId": "f2f16c41-6869-4c65-cd33-e150eddb38b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcyp5zjEkKUi"
      },
      "source": [
        "### Text Preprocessing\n",
        "To preprocess the text we will use the tokenizer and the text_to_sequences function from Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_FBKILpkKUi",
        "outputId": "de131b61-a3a0-4f3f-dd74-67acc9c690df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 95603 unique tokens\n",
            "Shape of data tensor: (808702, 15)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_index = tokenizer.word_index\n",
        "index2word = {v: k for k, v in word_index.items()}\n",
        "print('Found %s unique tokens' % len(word_index))\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', data_1.shape)\n",
        "NB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enY6ytHoRFs"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "numpy.random.shuffle(data_1)\n",
        "training, test = data_1[:int(len(data_1)*VALIDATION_SPLIT)], data_1[int(len(data_1)*VALIDATION_SPLIT):]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezdic1tbkKUj"
      },
      "source": [
        "### Sentence generator\n",
        "In order to reduce the memory requirements we will gradually read our sentences from the csv through Pandas as we feed them to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVbDOrZ7kKUk"
      },
      "outputs": [],
      "source": [
        "def sent_generator(chunksize):\n",
        "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)\n",
        "    for df in reader:\n",
        "        val3 = df.iloc[:,3:4].values.tolist()\n",
        "        val4 = df.iloc[:,4:5].values.tolist()\n",
        "        flat3 = [item for sublist in val3 for item in sublist]\n",
        "        flat4 = [str(item) for sublist in val4 for item in sublist]\n",
        "        texts = []\n",
        "        texts.extend(flat3[:])\n",
        "        texts.extend(flat4[:])\n",
        "\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        yield (data_train, data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu7LtxkBsVKQ",
        "outputId": "f8be1226-4d52-4ceb-f858-54b13fd791c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[   0,    2,    3, ...,  383,    8,   35],\n",
              "        [   0,    0,    0, ...,   10,    5, 4565],\n",
              "        [   0,    4,   13, ...,  146,    6, 2773],\n",
              "        ...,\n",
              "        [   0,    0,    0, ...,   22,    1,  140],\n",
              "        [   0,    0,    0, ...,   33, 6892,  730],\n",
              "        [   0,    0,    0, ...,    7,   52,  283]], dtype=int32),\n",
              " array([[   0,    2,    3, ...,  383,    8,   35],\n",
              "        [   0,    0,    0, ...,   10,    5, 4565],\n",
              "        [   0,    4,   13, ...,  146,    6, 2773],\n",
              "        ...,\n",
              "        [   0,    0,    0, ...,   22,    1,  140],\n",
              "        [   0,    0,    0, ...,   33, 6892,  730],\n",
              "        [   0,    0,    0, ...,    7,   52,  283]], dtype=int32))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(sent_generator(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwD7HBn7kKUl"
      },
      "source": [
        "### Word embeddings\n",
        "We will use pretrained Glove word embeddings as embeddings for our network. We create a matrix with one embedding for every word in our vocabulary and then we will pass this matrix as weights to the keras embedding layer of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8W-SqXWkKUl",
        "outputId": "698dbdb9-6bcc-43e3-b01b-cc6044e2465a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null word embeddings: 1\n"
          ]
        }
      ],
      "source": [
        "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i < NB_WORDS:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be the word embedding of 'unk'.\n",
        "            glove_embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp5H3ZILkKUm"
      },
      "source": [
        "### VAE model\n",
        "Our model is based on a seq2seq architecture with a bidirectional LSTM encoder and an LSTM decoder and SELU activations.\n",
        "We feed the latent representation at every timestep as input to the decoder through \"RepeatVector(max_len)\".\n",
        "\n",
        "We use the sum of the BCE loss on the final sentences generated + the KL loss from the Sampling layer.\n",
        "\n",
        "Moreover, due to the pandas iterator that reads the csv both the train size and validation size must be divisible by the batch_size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMXN-N8DRGwN"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "max_len = MAX_SEQUENCE_LENGTH\n",
        "emb_dim = EMBEDDING_DIM\n",
        "latent_dim = 32\n",
        "intermediate_dim = 96\n",
        "epsilon_std = 1.0\n",
        "num_sampled=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BBL36GsQNVM"
      },
      "outputs": [],
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Generates a random sample and combines with the encoder output\n",
        "\n",
        "    Args:\n",
        "      inputs -- output tensor from the encoder\n",
        "\n",
        "    Returns:\n",
        "      `inputs` tensors combined with a random sample\n",
        "    \"\"\"\n",
        "\n",
        "    # unpack the output of the encoder\n",
        "    mu, sigma = inputs\n",
        "\n",
        "    # get the size and dimensions of the batch\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "\n",
        "    # generate a random tensor\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "\n",
        "    # combine the inputs and noise\n",
        "    return mu + tf.exp(0.5 * sigma) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txuWVmNpQTxS"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "def encoder_layers(inputs, latent_dim):\n",
        "  \"\"\"Defines the encoder's layers.\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "\n",
        "  Returns:\n",
        "    mu -- learned mean\n",
        "    sigma -- learned standard deviation\n",
        "    batch_2.shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "\n",
        "  x_embed = Embedding(NB_WORDS, emb_dim, embeddings_initializer=Constant(glove_embedding_matrix), input_length=max_len, trainable=False, name='embedding')(inputs)\n",
        "  h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat', name='bidirectional_lstm_1')(x_embed)\n",
        "  h = Dropout(0.2)(h)\n",
        "  h = Dense(intermediate_dim, activation='relu')(h)\n",
        "  h = Dropout(0.2)(h)\n",
        "  mu = Dense(latent_dim, name='latent_mu')(h)\n",
        "  sigma = Dense(latent_dim, name='latent_sigma')(h)\n",
        "  return mu, sigma, intermediate_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mhim_RYREo_"
      },
      "outputs": [],
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "  \"\"\"Defines the encoder model with the Sampling layer\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    model -- the encoder model\n",
        "    conv_shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "\n",
        "  # declare the inputs tensor with the given shape\n",
        "  inputs = Input(shape=(max_len,))\n",
        "\n",
        "  # get the output of the encoder_layers() function\n",
        "  mu, sigma, shape = encoder_layers(inputs, latent_dim=latent_dim)\n",
        "\n",
        "  # feed mu and sigma to the Sampling layer\n",
        "  z = Sampling()((mu, sigma))\n",
        "\n",
        "  # build the whole encoder model\n",
        "  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "  return model, shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Hdr08PRdBQ"
      },
      "outputs": [],
      "source": [
        "def decoder_layers(inputs, shape):\n",
        "  \"\"\"Defines the decoder layers.\n",
        "  Args:\n",
        "    inputs -- output of the encoder\n",
        "    shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    tensor containing the decoded output\n",
        "  \"\"\"\n",
        "\n",
        "  x = RepeatVector(max_len)(inputs)\n",
        "  x = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)(x)\n",
        "  x = TimeDistributed(Dense(NB_WORDS, activation='softmax'))(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLaSgRJLSMb_"
      },
      "outputs": [],
      "source": [
        "def decoder_model(latent_dim, shape):\n",
        "  \"\"\"Defines the decoder model.\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    model -- the decoder model\n",
        "  \"\"\"\n",
        "\n",
        "  # set the inputs to the shape of the latent space\n",
        "  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "\n",
        "  # get the output of the decoder layers\n",
        "  outputs = decoder_layers(inputs, shape)\n",
        "\n",
        "  # declare the inputs and outputs of the model\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_6Z7DpvSU4J"
      },
      "outputs": [],
      "source": [
        "def kl_reconstruction_loss(mu, sigma):\n",
        "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "  Args:\n",
        "    mu -- mean\n",
        "    sigma -- standard deviation\n",
        "\n",
        "  Returns:\n",
        "    KLD loss\n",
        "  \"\"\"\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "\n",
        "  return kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AbhTK0KSYn1"
      },
      "outputs": [],
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  \"\"\"Defines the VAE model\n",
        "  Args:\n",
        "    encoder -- the encoder model\n",
        "    decoder -- the decoder model\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    the complete VAE model\n",
        "  \"\"\"\n",
        "\n",
        "  # set the inputs\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  # get mu, sigma, and z from the encoder output\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "\n",
        "  # get reconstructed output from the decoder\n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # define the inputs and outputs of the VAE\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "  # add the KL loss\n",
        "  kl_loss = kl_reconstruction_loss(mu, sigma)\n",
        "  model.add_loss(kl_loss)\n",
        "\n",
        "  return model, kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK_2u821Sea3"
      },
      "outputs": [],
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "  encoder, shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "  decoder = decoder_model(latent_dim=latent_dim, shape=shape)\n",
        "  vae, kl_loss = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "  return encoder, decoder, vae, kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7P47KcASkiK",
        "outputId": "ee2aa38f-2aef-4e46-b1e0-da62f77eac4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "encoder, decoder, vae, kl_loss = get_models(input_shape=(max_len), latent_dim=latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgN4iLKZTlYE"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "def custom_loss(y_true, y_pred):\n",
        "    print(y_true)\n",
        "    print(y_pred)\n",
        "    flattened_inputs = tf.cast(tf.reshape(y_true, shape=[-1]), dtype=tf.float32)\n",
        "    flattened_outputs = tf.cast(tf.reshape(tf.math.argmax(y_pred, axis=2), shape=[-1]), dtype=tf.float32)\n",
        "    bce_loss = tf.keras.losses.BinaryCrossentropy()(flattened_inputs, flattened_outputs) * max_len * batch_size\n",
        "    total_loss = bce_loss + kl_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzxHhagMagLf"
      },
      "outputs": [],
      "source": [
        "vae.compile(optimizer=optimizer, loss=custom_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IstJ6Xj3UAjt",
        "outputId": "95ce2a57-5421-4597-f913-fd9f117f9bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 15)]                 0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          [(None, 32),                 1376132   ['input_3[0][0]']             \n",
            "                              (None, 32),                                                         \n",
            "                              (None, 32)]                                                         \n",
            "                                                                                                  \n",
            " model_1 (Functional)        (None, 15, 12001)            1213633   ['model[0][2]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 32)                   0         ['model[0][1]']               \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.math.square (TFOpLambda  (None, 32)                   0         ['model[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLamb  (None, 32)                   0         ['tf.__operators__.add[0][0]',\n",
            " da)                                                                 'tf.math.square[0][0]']      \n",
            "                                                                                                  \n",
            " tf.math.exp (TFOpLambda)    (None, 32)                   0         ['model[0][1]']               \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLa  (None, 32)                   0         ['tf.math.subtract[0][0]',    \n",
            " mbda)                                                               'tf.math.exp[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  ()                           0         ['tf.math.subtract_1[0][0]']  \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLamb  ()                           0         ['tf.math.reduce_mean[0][0]'] \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)          ()                           0         ['tf.math.multiply[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2589765 (9.88 MB)\n",
            "Trainable params: 1389665 (5.30 MB)\n",
            "Non-trainable params: 1200100 (4.58 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1tRyWZsTm0O",
        "outputId": "f89527dc-cdf3-4b1c-877b-d08efff15c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7b64f15444c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7b64f15444c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 step: 0 mean loss = 0.026628846\n",
            "Epoch: 1 step: 100 mean loss = 9.698794e-07\n",
            "Epoch: 1 step: 200 mean loss = 5.805213e-07\n",
            "Epoch: 1 step: 300 mean loss = 6.2794425e-07\n",
            "Epoch: 1 step: 400 mean loss = 9.7826124e-08\n",
            "Epoch: 1 step: 500 mean loss = 4.656613e-09\n",
            "Epoch: 1 step: 600 mean loss = 4.5216643e-07\n",
            "Epoch: 1 step: 700 mean loss = 5.5879354e-09\n",
            "Epoch: 1 step: 800 mean loss = -4.656613e-09\n",
            "Epoch: 1 step: 900 mean loss = 1.8626451e-09\n",
            "Epoch: 1 step: 1000 mean loss = 8.965842e-08\n",
            "Start of epoch 2\n",
            "Epoch: 2 step: 0 mean loss = -2.7939677e-09\n",
            "Epoch: 2 step: 100 mean loss = 6.519258e-09\n",
            "Epoch: 2 step: 200 mean loss = 4.656613e-09\n",
            "Epoch: 2 step: 300 mean loss = 1.8626451e-09\n",
            "Epoch: 2 step: 400 mean loss = 7.450581e-09\n",
            "Epoch: 2 step: 500 mean loss = 9.313226e-10\n",
            "Epoch: 2 step: 600 mean loss = 1.38022e-08\n",
            "Epoch: 2 step: 700 mean loss = -9.313226e-10\n",
            "Epoch: 2 step: 800 mean loss = 4.656613e-09\n",
            "Epoch: 2 step: 900 mean loss = 1.8626451e-09\n",
            "Epoch: 2 step: 1000 mean loss = 6.519258e-09\n"
          ]
        }
      ],
      "source": [
        "# # Training loop.\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch + 1,))\n",
        "\n",
        "  # iterate over the batches of the dataset.\n",
        "  for step, (x_train, x_train) in enumerate(sent_generator(batch_size/2)):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # feed a batch to the VAE model\n",
        "      reconstructed = vae(tf.constant(x_train))\n",
        "      # add KLD regularization loss\n",
        "      loss = vae.losses\n",
        "\n",
        "    # get the gradients and update the weights\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(\n",
        "    (grad, var)\n",
        "    for (grad, var) in zip(grads, vae.trainable_variables)\n",
        "    if grad is not None\n",
        "    )\n",
        "    loss_metric = tf.keras.metrics.Mean()\n",
        "    # compute the loss metric\n",
        "    loss_metric(loss)\n",
        "\n",
        "    # display outputs every 100 steps\n",
        "    if step % 100 == 0:\n",
        "      print('Epoch: %s step: %s mean loss = %s' % (epoch + 1, step, loss_metric.result().numpy()))\n",
        "    if step % 1000 == 0 and step != 0:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e63vH-Z-6jm-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vfhd3AKUjuci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP5AYpaUkKUo"
      },
      "source": [
        "### Project and sample sentences from the latent space\n",
        "Now we build an encoder model model that takes a sentence and projects it on the latent space and a decoder model that goes from the latent space back to the text representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeGHwhePkKUo"
      },
      "source": [
        "### Test on validation sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nm0_561kKUo"
      },
      "outputs": [],
      "source": [
        "index2word = {v: k for k, v in word_index.items()}\n",
        "sent_encoded = encoder.predict(test[:10000], batch_size = 16)\n",
        "x_test_reconstructed = decoder.predict(sent_encoded[0])\n",
        "\n",
        "sent_idx = 672\n",
        "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
        "#np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n",
        "#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n",
        "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
        "word_list\n",
        "original_sent = list(np.vectorize(index2word.get)(test[sent_idx]))\n",
        "original_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haITKNDN6UBd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}