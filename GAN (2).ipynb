{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gQPeZsvuk5j5"
      },
      "outputs": [],
      "source": [
        "# Importing the Libraries\n",
        "#The code initializes the environment for a deep learning task in Python using PyTorch.\n",
        "#It imports necessary libraries and sets the computation to run on GPU if available, or CPU otherwise, for efficient processing.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a basic transform\n",
        "#This code defines a transformation pipeline for image data,\n",
        "#converting images to PyTorch tensors and normalizing their pixel values with mean 0.5 and standard deviation 0.5 for each RGB channel.\n",
        "transform = transforms.Compose([\n",
        "\ttransforms.ToTensor(),\n",
        "\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "CUsNcHBsk-Jp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "#This code creates a training dataset and a data loader for the CIFAR-10 dataset in PyTorch.\n",
        "# The datasets.CIFAR10 function loads the CIFAR-10 dataset, specifying a directory for the data, enabling training data (train=True),\n",
        "# downloading the data if not present (download=True), and applying the previously defined transformations (transform=transform).\n",
        "# The DataLoader wraps the dataset, setting a batch size of 32 (number of images processed in one step) and\n",
        "# shuffling the data (shuffle=True) for each training iteration to promote model generalization.\n",
        "train_dataset = datasets.CIFAR10(root='./data',\\\n",
        "\t\t\ttrain=True, download=True, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, \\\n",
        "\t\t\t\t\t\t\t\tbatch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Gii-CalQuG",
        "outputId": "ec350449-28ae-4237-beb7-f7c9c61d6240"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29076196.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "# This code defines hyperparameters for training a machine learning model, likely a GAN.\n",
        "# latent_dim is the dimensionality of the input noise vector (100),\n",
        "#  lr is the learning rate (0.0002) for the optimizer, beta1 and beta2 are parameters\n",
        "#  for the Adam optimizer influencing momentum (0.5) and scaling (0.999), and num_epochs sets the number of training cycles through the entire dataset (10).\n",
        "latent_dim = 100\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "num_epochs = 10\n"
      ],
      "metadata": {
        "id": "MrquwNKjlcI0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#This code defines a Generator class for a GAN using PyTorch.\n",
        "#It extends nn.Module and initializes a sequential model transforming a latent space vector into an image.\n",
        "#The process involves linear expansion, reshaping, upscaling, and convolutional layers with ReLU activation and batch normalization,\n",
        "#finally outputting an image with a Tanh activation function. The forward method defines the data flow through the network.\n",
        "\n",
        "# Define the generator\n",
        "class Generator(nn.Module):\n",
        "\tdef __init__(self, latent_dim):\n",
        "\t\tsuper(Generator, self).__init__()\n",
        "\n",
        "\t\tself.model = nn.Sequential(\n",
        "\t\t\tnn.Linear(latent_dim, 128 * 8 * 8),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Unflatten(1, (128, 8, 8)),\n",
        "\t\t\tnn.Upsample(scale_factor=2),\n",
        "\t\t\tnn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "\t\t\tnn.BatchNorm2d(128, momentum=0.78),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Upsample(scale_factor=2),\n",
        "\t\t\tnn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "\t\t\tnn.BatchNorm2d(64, momentum=0.78),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "\t\t\tnn.Tanh()\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, z):\n",
        "\t\timg = self.model(z)\n",
        "\t\treturn img\n"
      ],
      "metadata": {
        "id": "d_AeZsLfmBwH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for Discriminator\n",
        "# Define the discriminator\n",
        "#This code defines a Discriminator class, part of a Generative Adversarial Network (GAN) in PyTorch.\n",
        "#It extends nn.Module and comprises a neural network that classifies images as real or fake.\n",
        "#The network uses convolutional layers with LeakyReLU activation and dropout for regularization, batch normalization for stability,\n",
        "#and a final linear layer with a sigmoid activation to output a probability indicating the image's authenticity.\n",
        "#The forward method specifies the data flow through this network.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Discriminator, self).__init__()\n",
        "\n",
        "\t\tself.model = nn.Sequential(\n",
        "\t\tnn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "\t\tnn.LeakyReLU(0.2),\n",
        "\t\tnn.Dropout(0.25),\n",
        "\t\tnn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "\t\tnn.ZeroPad2d((0, 1, 0, 1)),\n",
        "\t\tnn.BatchNorm2d(64, momentum=0.82),\n",
        "\t\tnn.LeakyReLU(0.25),\n",
        "\t\tnn.Dropout(0.25),\n",
        "\t\tnn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "\t\tnn.BatchNorm2d(128, momentum=0.82),\n",
        "\t\tnn.LeakyReLU(0.2),\n",
        "\t\tnn.Dropout(0.25),\n",
        "\t\tnn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "\t\tnn.BatchNorm2d(256, momentum=0.8),\n",
        "\t\tnn.LeakyReLU(0.25),\n",
        "\t\tnn.Dropout(0.25),\n",
        "\t\tnn.Flatten(),\n",
        "\t\tnn.Linear(256 * 5 * 5, 1),\n",
        "\t\tnn.Sigmoid()\n",
        "\t)\n",
        "\n",
        "\tdef forward(self, img):\n",
        "\t\tvalidity = self.model(img)\n",
        "\t\treturn validity\n"
      ],
      "metadata": {
        "id": "h1pdPI74mTaT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the GAN\n",
        "\n",
        "# Define the generator and discriminator\n",
        "#This code initializes a GAN's Generator and Discriminator on a specified device (GPU/CPU).\n",
        "#It sets up a binary cross-entropy loss function for training.\n",
        "#Adam optimizers are created for both models with predefined learning rates and momentum parameters,\n",
        "#optimizing the network's performance during training.\n",
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters()\\\n",
        "\t\t\t\t\t\t, lr=lr, betas=(beta1, beta2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters()\\\n",
        "\t\t\t\t\t\t, lr=lr, betas=(beta1, beta2))\n"
      ],
      "metadata": {
        "id": "pVFnJIu2mnKN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "# This code is a training loop for a Generative Adversarial Network (GAN) in PyTorch.\n",
        "#  It alternates between training the Discriminator to distinguish real from fake images and training the Generator to create increasingly realistic images.\n",
        "#  Each epoch processes batches of real images, computes losses for both models, and updates them using backpropagation.\n",
        "#  The progress is monitored by printing losses, and at the end of each epoch, a set of generated images is displayed to visually evaluate\n",
        "#  the Generator's performance.\n",
        "# This iterative process aims to enhance the Generator's image creation and the Discriminator's classification accuracy.\n",
        "for epoch in range(num_epochs):\n",
        "\tfor i, batch in enumerate(dataloader):\n",
        "\t# Convert list to tensor\n",
        "\t\treal_images = batch[0].to(device)\n",
        "\n",
        "\t\t# Adversarial ground truths\n",
        "\t\tvalid = torch.ones(real_images.size(0), 1, device=device)\n",
        "\t\tfake = torch.zeros(real_images.size(0), 1, device=device)\n",
        "\n",
        "\t\t# Configure input\n",
        "\t\treal_images = real_images.to(device)\n",
        "\n",
        "\t\t# ---------------------\n",
        "\t\t# Train Discriminator\n",
        "\t\t# ---------------------\n",
        "\n",
        "\t\toptimizer_D.zero_grad()\n",
        "\n",
        "\t\t# Sample noise as generator input\n",
        "\t\tz = torch.randn(real_images.size(0), latent_dim, device=device)\n",
        "\n",
        "\t\t# Generate a batch of images\n",
        "\t\tfake_images = generator(z)\n",
        "\n",
        "\t\t# Measure discriminator's ability\n",
        "\t\t# to classify real and fake images\n",
        "\t\treal_loss = adversarial_loss(discriminator\\\n",
        "\t\t\t\t\t\t\t\t\t(real_images), valid)\n",
        "\t\tfake_loss = adversarial_loss(discriminator\\\n",
        "\t\t\t\t\t\t\t\t\t(fake_images.detach()), fake)\n",
        "\t\td_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "\t\t# Backward pass and optimize\n",
        "\t\td_loss.backward()\n",
        "\t\toptimizer_D.step()\n",
        "\n",
        "\t\t# -----------------\n",
        "\t\t# Train Generator\n",
        "\t\t# -----------------\n",
        "\n",
        "\t\toptimizer_G.zero_grad()\n",
        "\n",
        "\t\t# Generate a batch of images\n",
        "\t\tgen_images = generator(z)\n",
        "\n",
        "\t\t# Adversarial loss\n",
        "\t\tg_loss = adversarial_loss(discriminator(gen_images), valid)\n",
        "\n",
        "\t\t# Backward pass and optimize\n",
        "\t\tg_loss.backward()\n",
        "\t\toptimizer_G.step()\n",
        "\n",
        "\t\t# ---------------------\n",
        "\t\t# Progress Monitoring\n",
        "\t\t# ---------------------\n",
        "\n",
        "\t\tif (i + 1) % 100 == 0:\n",
        "\t\t\tprint(\n",
        "\t\t\t\tf\"Epoch [{epoch+1}/{num_epochs}]\\\n",
        "\t\t\t\t\t\tBatch {i+1}/{len(dataloader)} \"\n",
        "\t\t\t\tf\"Discriminator Loss: {d_loss.item():.4f} \"\n",
        "\t\t\t\tf\"Generator Loss: {g_loss.item():.4f}\"\n",
        "\t\t\t)\n",
        "\n",
        "\t# Save generated images for every epoch\n",
        "\tif (epoch + 1) % 10 == 0:\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tz = torch.randn(16, latent_dim, device=device)\n",
        "\t\t\tgenerated = generator(z).detach().cpu()\n",
        "\t\t\tgrid = torchvision.utils.make_grid(generated,\\\n",
        "\t\t\t\t\t\t\t\t\t\tnrow=4, normalize=True)\n",
        "\t\t\tplt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "\t\t\tplt.axis(\"off\")\n",
        "\t\t\tplt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxQlJoxlmt3p",
        "outputId": "08b678f6-7033-4037-bdb1-025ada8aa876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.3521 Generator Loss: 1.1510\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.6507 Generator Loss: 1.6493\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.3588 Generator Loss: 1.5329\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 1.0045 Generator Loss: 0.4291\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.4074 Generator Loss: 1.2222\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.6612 Generator Loss: 1.1582\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.5995 Generator Loss: 0.8287\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.6310 Generator Loss: 1.4543\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.4429 Generator Loss: 1.7726\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.5140 Generator Loss: 2.2239\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.5708 Generator Loss: 0.8809\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.5351 Generator Loss: 1.4202\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.4496 Generator Loss: 1.6928\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.3458 Generator Loss: 1.0976\n",
            "Epoch [1/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.5511 Generator Loss: 1.5317\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.5604 Generator Loss: 1.7385\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.6854 Generator Loss: 0.8119\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.5567 Generator Loss: 1.1779\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.3385 Generator Loss: 1.2177\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.4406 Generator Loss: 1.4508\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.3946 Generator Loss: 0.8941\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.6272 Generator Loss: 2.0077\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.5189 Generator Loss: 1.8412\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.4352 Generator Loss: 1.7241\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.4215 Generator Loss: 1.5253\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.7182 Generator Loss: 0.8646\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.6265 Generator Loss: 1.3171\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.4517 Generator Loss: 1.2930\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.7467 Generator Loss: 2.0017\n",
            "Epoch [2/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.3951 Generator Loss: 0.9094\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.5656 Generator Loss: 2.2812\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.3631 Generator Loss: 1.6895\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.7067 Generator Loss: 1.2474\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.5732 Generator Loss: 1.7020\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.4496 Generator Loss: 0.9690\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.3255 Generator Loss: 1.6457\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.7685 Generator Loss: 0.8843\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.5366 Generator Loss: 2.9968\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.5010 Generator Loss: 1.1308\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.3771 Generator Loss: 1.4364\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.7510 Generator Loss: 1.2503\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.6946 Generator Loss: 1.2554\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.7530 Generator Loss: 1.1004\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.6692 Generator Loss: 1.6233\n",
            "Epoch [3/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.7779 Generator Loss: 0.8752\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.4820 Generator Loss: 1.8307\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.7346 Generator Loss: 0.6777\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.3582 Generator Loss: 1.4200\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.6937 Generator Loss: 1.0966\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.1991 Generator Loss: 1.7063\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.6823 Generator Loss: 1.3002\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.3495 Generator Loss: 1.3841\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.7569 Generator Loss: 1.4217\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.4162 Generator Loss: 1.0012\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.5498 Generator Loss: 1.1279\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.4461 Generator Loss: 0.8837\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.3721 Generator Loss: 1.3529\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.7274 Generator Loss: 1.1244\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.6424 Generator Loss: 2.1715\n",
            "Epoch [4/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.4379 Generator Loss: 1.5763\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.6885 Generator Loss: 0.7494\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.3457 Generator Loss: 1.1515\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.6005 Generator Loss: 0.9584\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.5084 Generator Loss: 0.9682\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.7735 Generator Loss: 0.7392\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.4272 Generator Loss: 0.8798\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.5203 Generator Loss: 0.8509\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.6931 Generator Loss: 1.3271\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.3273 Generator Loss: 1.0674\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.6708 Generator Loss: 1.4851\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.6040 Generator Loss: 1.1215\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.5192 Generator Loss: 1.3314\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.6069 Generator Loss: 1.2825\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.6414 Generator Loss: 1.2468\n",
            "Epoch [5/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.3284 Generator Loss: 1.8208\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.5624 Generator Loss: 0.4851\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.5016 Generator Loss: 1.3358\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.5920 Generator Loss: 1.0609\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.6593 Generator Loss: 1.4373\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.6702 Generator Loss: 2.0233\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.5751 Generator Loss: 1.2504\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.6143 Generator Loss: 2.0710\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.5942 Generator Loss: 1.4630\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.5054 Generator Loss: 0.8927\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.4042 Generator Loss: 1.5440\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.6784 Generator Loss: 1.4448\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.7557 Generator Loss: 1.4645\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.7330 Generator Loss: 1.3446\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.3949 Generator Loss: 1.6169\n",
            "Epoch [6/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.5152 Generator Loss: 1.7815\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.5788 Generator Loss: 1.2316\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.4401 Generator Loss: 1.5686\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.6080 Generator Loss: 1.0679\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.5923 Generator Loss: 2.0438\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.5809 Generator Loss: 1.0362\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.4932 Generator Loss: 1.0826\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.3526 Generator Loss: 1.7145\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.6721 Generator Loss: 1.7838\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.5294 Generator Loss: 1.2556\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.6152 Generator Loss: 0.9605\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.4383 Generator Loss: 0.8679\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.6243 Generator Loss: 0.8110\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.3405 Generator Loss: 1.9282\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.5182 Generator Loss: 0.8713\n",
            "Epoch [7/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.6275 Generator Loss: 1.1336\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.8114 Generator Loss: 1.6112\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.6669 Generator Loss: 0.7263\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.6331 Generator Loss: 0.9403\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.5035 Generator Loss: 1.1796\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.5665 Generator Loss: 1.5945\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.3699 Generator Loss: 1.8341\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.4520 Generator Loss: 1.4988\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.4169 Generator Loss: 1.5254\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.4224 Generator Loss: 0.7315\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.8802 Generator Loss: 1.5113\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.5304 Generator Loss: 0.9298\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.5761 Generator Loss: 1.1585\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.6002 Generator Loss: 2.1958\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 1400/1563 Discriminator Loss: 0.7888 Generator Loss: 0.8732\n",
            "Epoch [8/10]\t\t\t\t\t\tBatch 1500/1563 Discriminator Loss: 0.4965 Generator Loss: 1.1946\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 100/1563 Discriminator Loss: 0.6278 Generator Loss: 1.8054\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 200/1563 Discriminator Loss: 0.4367 Generator Loss: 0.9978\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 300/1563 Discriminator Loss: 0.8921 Generator Loss: 1.3697\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 400/1563 Discriminator Loss: 0.4533 Generator Loss: 0.5367\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 500/1563 Discriminator Loss: 0.8031 Generator Loss: 0.9239\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 600/1563 Discriminator Loss: 0.6938 Generator Loss: 1.2225\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 700/1563 Discriminator Loss: 0.8194 Generator Loss: 1.2189\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 800/1563 Discriminator Loss: 0.6650 Generator Loss: 1.4876\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 900/1563 Discriminator Loss: 0.5702 Generator Loss: 0.8778\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 1000/1563 Discriminator Loss: 0.4131 Generator Loss: 1.2578\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 1100/1563 Discriminator Loss: 0.6655 Generator Loss: 1.6168\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 1200/1563 Discriminator Loss: 0.7960 Generator Loss: 1.0242\n",
            "Epoch [9/10]\t\t\t\t\t\tBatch 1300/1563 Discriminator Loss: 0.6275 Generator Loss: 1.8380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djOiEZAwm1ga"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}